{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oyiuQFFkFyJ5"
      },
      "source": [
        "Version: 07.01.2022"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CiqD9WxzFyJ8"
      },
      "source": [
        "# Exercise 1 Part 1: Processing Text\n",
        "\n",
        "In this lab, you will look at simple techniques to clean and prepare text data for modeling with machine learning (ML).\n",
        "\n",
        "\n",
        "## Lab steps\n",
        "\n",
        "To complete this lab, you will follow these steps:\n",
        "\n",
        "1. [Working with simple text-cleaning processes](#1.-Working-with-simple-text-cleaning-processes)\n",
        "2. [Working with lexicon-based text processing](#2.-Working-with-lexicon-based-text-processing)\n",
        "        \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wExlTjbiFyJ8",
        "outputId": "a4cbf0f4-a600-4911-bad7-e49f312927e9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
            "Requirement already satisfied: pip in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (22.3)\n",
            "Collecting pip\n",
            "  Downloading pip-23.0.1-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m77.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pip\n",
            "  Attempting uninstall: pip\n",
            "    Found existing installation: pip 22.3\n",
            "    Uninstalling pip-22.3:\n",
            "      Successfully uninstalled pip-22.3\n",
            "Successfully installed pip-23.0.1\n",
            "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
            "Requirement already satisfied: scikit-learn in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (1.1.3)\n",
            "Collecting scikit-learn\n",
            "  Downloading scikit_learn-1.2.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.6/9.6 MB\u001b[0m \u001b[31m71.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: joblib>=1.1.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from scikit-learn) (1.2.0)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from scikit-learn) (1.22.4)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from scikit-learn) (1.9.3)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from scikit-learn) (3.1.0)\n",
            "Installing collected packages: scikit-learn\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.1.3\n",
            "    Uninstalling scikit-learn-1.1.3:\n",
            "      Successfully uninstalled scikit-learn-1.1.3\n",
            "Successfully installed scikit-learn-1.2.1\n",
            "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
            "Requirement already satisfied: sagemaker in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (2.128.0)\n",
            "Collecting sagemaker\n",
            "  Downloading sagemaker-2.133.0.tar.gz (671 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m671.5/671.5 kB\u001b[0m \u001b[31m51.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25hRequirement already satisfied: attrs<23,>=20.3.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker) (22.1.0)\n",
            "Requirement already satisfied: boto3<2.0,>=1.26.28 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker) (1.26.46)\n",
            "Requirement already satisfied: google-pasta in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker) (0.2.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.9.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker) (1.22.4)\n",
            "Requirement already satisfied: protobuf<4.0,>=3.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker) (3.20.3)\n",
            "Requirement already satisfied: protobuf3-to-dict<1.0,>=0.1.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker) (0.1.5)\n",
            "Requirement already satisfied: smdebug_rulesconfig==1.0.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker) (1.0.1)\n",
            "Requirement already satisfied: importlib-metadata<5.0,>=1.4.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker) (4.13.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker) (21.3)\n",
            "Requirement already satisfied: pandas in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker) (1.5.1)\n",
            "Requirement already satisfied: pathos in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker) (0.3.0)\n",
            "Requirement already satisfied: schema in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker) (0.7.5)\n",
            "Collecting botocore<1.30.0,>=1.29.46\n",
            "  Downloading botocore-1.29.74-py3-none-any.whl (10.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.4/10.4 MB\u001b[0m \u001b[31m64.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from boto3<2.0,>=1.26.28->sagemaker) (0.6.0)\n",
            "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from boto3<2.0,>=1.26.28->sagemaker) (0.10.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from importlib-metadata<5.0,>=1.4.0->sagemaker) (3.10.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from packaging>=20.0->sagemaker) (3.0.9)\n",
            "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from protobuf3-to-dict<1.0,>=0.1.5->sagemaker) (1.16.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pandas->sagemaker) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pandas->sagemaker) (2022.5)\n",
            "Requirement already satisfied: pox>=0.3.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pathos->sagemaker) (0.3.2)\n",
            "Requirement already satisfied: ppft>=1.7.6.6 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pathos->sagemaker) (1.7.6.6)\n",
            "Requirement already satisfied: dill>=0.3.6 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pathos->sagemaker) (0.3.6)\n",
            "Requirement already satisfied: multiprocess>=0.70.14 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pathos->sagemaker) (0.70.14)\n",
            "Requirement already satisfied: contextlib2>=0.5.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from schema->sagemaker) (21.6.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from botocore<1.30.0,>=1.29.46->boto3<2.0,>=1.26.28->sagemaker) (1.26.8)\n",
            "Building wheels for collected packages: sagemaker\n",
            "  Building wheel for sagemaker (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Created wheel for sagemaker: filename=sagemaker-2.133.0-py2.py3-none-any.whl size=908931 sha256=f0291f61a14caa873ceab74ef415c51b8a13cf354c6e6973627c4aee46b1b3ad\n",
            "  Stored in directory: /home/ec2-user/.cache/pip/wheels/6b/25/3c/b946d2bba8d185877d54287263610f7bae22584935c913fa65\n",
            "Successfully built sagemaker\n",
            "Installing collected packages: botocore, sagemaker\n",
            "  Attempting uninstall: botocore\n",
            "    Found existing installation: botocore 1.24.19\n",
            "    Uninstalling botocore-1.24.19:\n",
            "      Successfully uninstalled botocore-1.24.19\n",
            "  Attempting uninstall: sagemaker\n",
            "    Found existing installation: sagemaker 2.128.0\n",
            "    Uninstalling sagemaker-2.128.0:\n",
            "      Successfully uninstalled sagemaker-2.128.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "awscli 1.27.46 requires botocore==1.29.46, but you have botocore 1.29.74 which is incompatible.\n",
            "aiobotocore 2.0.1 requires botocore<1.22.9,>=1.22.8, but you have botocore 1.29.74 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed botocore-1.29.74 sagemaker-2.133.0\n"
          ]
        }
      ],
      "source": [
        "#Upgrade dependencies\n",
        "!pip install --upgrade pip\n",
        "!pip install --upgrade scikit-learn\n",
        "!pip install --upgrade sagemaker"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p607X-q8FyJ-"
      },
      "source": [
        "## 1. <a name=\"1\">Working with simple text-cleaning processes</a>\n",
        "([Go to top](#Lab-3.2:-Processing-Text))\n",
        "\n",
        "In this section, you will do some general-purpose text cleaning. The following methods for cleaning can be extended, depending on the application."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6gEjdmY5FyJ-",
        "outputId": "587550fc-29e0-48e0-f8f7-e1d956c85f4e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   This is a message to be cleaned. It might involve some things like: <br>, ?, :, ''  adjacent spaces, and tabs     .  \n"
          ]
        }
      ],
      "source": [
        "text = \"   This is a message to be cleaned. It might involve some things like: <br>, ?, :, ''  adjacent spaces, and tabs     .  \"\n",
        "print(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C4wlH5aKFyJ_"
      },
      "source": [
        "First, change the text so that it's all lowercase:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LmRQprFyFyJ_",
        "outputId": "50124379-2d31-4989-9edb-0ab1e0d5be59"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   this is a message to be cleaned. it might involve some things like: <br>, ?, :, ''  adjacent spaces, and tabs     .  \n"
          ]
        }
      ],
      "source": [
        "text = text.lower()\n",
        "print(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yy-8DKQNFyJ_"
      },
      "source": [
        "Next, remove leading whitespace or trailing whitespace with the following code:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iwoP3TIaFyKA",
        "outputId": "ba4a56a7-af3b-43bd-9309-e967d32b0e4c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "this is a message to be cleaned. it might involve some things like: <br>, ?, :, ''  adjacent spaces, and tabs     .\n"
          ]
        }
      ],
      "source": [
        "text = text.strip()\n",
        "print(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y1jHOMugFyKA"
      },
      "source": [
        "Use a regular expression to remove HTML tags or markup:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8LutPRcWFyKB",
        "outputId": "31cdcea3-cebc-4fc5-a199-b8c2c53411f0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "this is a message to be cleaned. it might involve some things like: , ?, :, ''  adjacent spaces, and tabs     .\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "text = re.compile('<.*?>').sub('', text)\n",
        "print(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rDNJXrtnFyKB"
      },
      "source": [
        "Replace punctuation with a space. Be careful with this task. Depending on the application, punctuation can actually be useful. For example, punctuation might affect the positive or negative meaning of a sentence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C2mbcCQIFyKB",
        "outputId": "5fdf8c4c-cfaf-4e6a-97e3-fc8eec92f2a5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "this is a message to be cleaned  it might involve some things like              adjacent spaces  and tabs      \n"
          ]
        }
      ],
      "source": [
        "import re, string\n",
        "\n",
        "text = re.compile('[%s]' % re.escape(string.punctuation)).sub(' ', text)\n",
        "print(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aZ6Df3NXFyKB"
      },
      "source": [
        "Remove any extra spaces and tabs:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gDt81VV8FyKC",
        "outputId": "af1f9ab0-90b9-40f8-ad32-581a6fe6c92b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "this is a message to be cleaned it might involve some things like adjacent spaces and tabs \n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "text = re.sub('\\s+', ' ', text)\n",
        "print(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mhiX0lvmFyKC"
      },
      "source": [
        "## 2. Working with lexicon-based text processing\n",
        "([Go to top](#Lab-3.2:-Processing-Text))\n",
        "\n",
        "In the previous section, you used some general-purpose text pre-processing methods. Lexicon-based methods are usually applied *after* the common text-processing methods. They are used to normalize sentences in the dataset. *Normalization* means putting words into a similar format that will also enhance the similarities (if any) between sentences."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bgLZsuxpFyKC"
      },
      "source": [
        "For this example, you must install some packages:\n",
        "\n",
        "- punkt - A pretrained sentence tokenizer for the English language\n",
        "- averaged_perceptron_tagger - A part-of-sentence (POS) tagger\n",
        "- wordnet - A large database of English words that can be used to find the meanings of words, synonyms, antonyms, and more\n",
        "\n",
        "Run the following cell:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XGA5xNG-FyKC",
        "outputId": "535229e5-bab7-40ce-8a0a-7ca175a7be14"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /home/ec2-user/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /home/ec2-user/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package wordnet to /home/ec2-user/nltk_data...\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b3mjmoSfFyKD"
      },
      "source": [
        "#### Stopword removal\n",
        "Some words in sentences can occur very frequently, and they don't contribute too much to the overall meaning of the sentences. Typically, you would use list of these words and remove them from each sentence. For example, stopwords include: *a*, *an*, *the*, *this*, *that*, *is*, *it*, *to*, and *and*."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BeyCiZg7FyKD"
      },
      "outputs": [],
      "source": [
        "# Use a tokenizer from the NLTK library\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "filtered_sentence = []\n",
        "\n",
        "# Stopword lists can be adjusted for your problem\n",
        "stopwords = [\"a\", \"an\", \"the\", \"this\", \"that\", \"is\", \"it\", \"to\", \"and\"]\n",
        "\n",
        "# Tokenize the sentence\n",
        "words = word_tokenize(text)\n",
        "for w in words:\n",
        "    if w not in stopwords:\n",
        "        filtered_sentence.append(w)\n",
        "text = \" \".join(filtered_sentence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JaOaNfcyFyKD",
        "outputId": "44fb7d30-c402-4b21-80ba-7f90337ba0ac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "message be cleaned might involve some things like adjacent spaces tabs\n"
          ]
        }
      ],
      "source": [
        "print(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mz94PuIPFyKE"
      },
      "source": [
        "#### Stemming words\n",
        "Stemming is a rule-based system for converting words into their root form. It removes suffixes from words. This process helps enhance similarities (if any) between sentences. \n",
        "\n",
        "Examples:\n",
        "\n",
        "\"jumping\", \"jumped\" -> \"jump\"\n",
        "\n",
        "\"cars\" -> \"car\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lMDjX9PrFyKE"
      },
      "outputs": [],
      "source": [
        "# Use a tokenizer and stemmer from the NLTK library\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import SnowballStemmer\n",
        "\n",
        "# Initialize the stemmer\n",
        "snow = SnowballStemmer('english')\n",
        "\n",
        "stemmed_sentence = []\n",
        "# Tokenize the sentence\n",
        "words = word_tokenize(text)\n",
        "for w in words:\n",
        "    # Stem the word/token\n",
        "    stemmed_sentence.append(snow.stem(w))\n",
        "stemmed_text = \" \".join(stemmed_sentence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TXj9Ix8OFyKE",
        "outputId": "d1b0f419-c5ee-4fa4-8dd1-1d5e746f7e12"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "messag be clean might involv some thing like adjac space tab\n"
          ]
        }
      ],
      "source": [
        "print(stemmed_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xPEwXkqSFyKE"
      },
      "source": [
        "From the previous cell, you can see that the stemming operation is *not* perfect. It generated some mistakes, such as *messag*, *involv*, and *adjac*. Stemming is a rule-based method that sometimes mistakenly remove suffixes from words. Nevertheless, it runs quickly."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CD7aeQklFyKF"
      },
      "source": [
        "#### Lemmatizing words\n",
        "If you are not satisfied with the result of stemming, you can use lemmatization instead. It usually requires more work, but it gives better results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5zba2FgOFyKF",
        "outputId": "21fb61b7-8137-4105-a9ad-5bcfbccc405a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package omw-1.4 to /home/ec2-user/nltk_data...\n"
          ]
        }
      ],
      "source": [
        "# Importing the necessary functions\n",
        "import nltk\n",
        "nltk.download('omw-1.4')\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Initialize the lemmatizer\n",
        "wl = WordNetLemmatizer()\n",
        "\n",
        "# This is a helper function to map NTLK position tags\n",
        "# Full list is available here: https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html\n",
        "def get_wordnet_pos(tag):\n",
        "    if tag.startswith('J'):\n",
        "        return wordnet.ADJ\n",
        "    elif tag.startswith('V'):\n",
        "        return wordnet.VERB\n",
        "    elif tag.startswith('N'):\n",
        "        return wordnet.NOUN\n",
        "    elif tag.startswith('R'):\n",
        "        return wordnet.ADV\n",
        "    else:\n",
        "        return wordnet.NOUN\n",
        "\n",
        "lemmatized_sentence = []\n",
        "# Tokenize the sentence\n",
        "words = word_tokenize(text)\n",
        "# Get position tags\n",
        "word_pos_tags = nltk.pos_tag(words)\n",
        "# Map the position tag and lemmatize the word or token\n",
        "for idx, tag in enumerate(word_pos_tags):\n",
        "    lemmatized_sentence.append(wl.lemmatize(tag[0], get_wordnet_pos(tag[1])))\n",
        "\n",
        "lemmatized_text = \" \".join(lemmatized_sentence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j6qhyfBJFyKF",
        "outputId": "1ff7ff89-c0ff-46a9-b5f2-f39c93de4d2e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "message be clean might involve some thing like adjacent space tabs\n"
          ]
        }
      ],
      "source": [
        "print(lemmatized_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o3tMrX-0FyKF"
      },
      "source": [
        "You can use the tasks you completed in this notebook for many of the business problems that you will work on in this course. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sXKajW6xFyKF"
      },
      "source": [
        "# Congratulations!\n",
        "\n",
        "You have completed this lab, and you can now end the lab by following the lab guide instructions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vaZRvCxAFyKF"
      },
      "source": [
        "*©2021 Amazon Web Services, Inc. or its affiliates. All rights reserved. This work may not be reproduced or redistributed, in whole or in part, without prior written permission from Amazon Web Services, Inc. Commercial copying, lending, or selling is prohibited. All trademarks are the property of their owners.*"
      ]
    }
  ],
  "metadata": {
    "instance_type": "ml.t3.medium",
    "kernelspec": {
      "display_name": "conda_python3",
      "language": "python",
      "name": "conda_python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}